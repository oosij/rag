{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd3e603-94bd-4d46-a5eb-949033a65c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import cx_Oracle\n",
    "#import pymysql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "#import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "\n",
    "import re\n",
    "import os \n",
    "import openai\n",
    "import docx\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from kss import split_sentences  \n",
    "from konlpy.tag import Kkma\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff39c358-e21b-4ff7-890d-bd9e4793068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_paths(folder_path):\n",
    "    pdf_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                pdf_paths.append(os.path.join(root, file))\n",
    "    return pdf_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e7aeae-3485-4d8f-b62f-480c35c42ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/adobe_pdf/json_to_txt\\\\BNK투자증권_포스코퓨처엠_20240409_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\DS투자증권_포스코퓨처엠_20230428_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\교보증권_포스코퓨처엠_20240202_003670_20190031_342_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\상상인_기업분석_포스코퓨처엠_20240425_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\신한투자증권_포스코퓨처엠_240426_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\유안타증권_포스코퓨처엠_240507_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\키움증권_포스코퓨처엠_240426_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\포스코퓨처엠_IBK투자증권_20231025_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\포스코퓨처엠_대신증권_20231025_20240703.txt',\n",
       " './data/adobe_pdf/json_to_txt\\\\하나증권_포스코퓨처엠_20240426_20240703.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = './data/adobe_pdf/json_to_txt'\n",
    "# 전처리 한 데이터 : 표 / 이미지 및 불필요한 단락 제거 \n",
    "txt_list = get_txt_paths(root_path)\n",
    "txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5639d122-40ef-42f5-b8fb-91c4f26ae009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(root_path, glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "documents = loader.load()\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15ffa69-a07b-4a2f-b4de-4e521789a170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='하반기에는 실적 개선 + 원재료 수직계열화 시작\\n재고평가손 환입이 더해져 1Q 실적은 예상보다 양호할 전망 1Q 연결OP 355억원으로 예상보다 양호할 전망이다. 시장 성장 둔화와 ASP 하락으로 에너지소재의 본원 수익성은 낮을 것으로 예상되나, 23.4Q에 인식 된 양극재 재고평가손실 중 일부분이 환입되면서 헤드라인 손익은 예상을 상 회할 전망이다. 손실환입까지 반영한 양극재OP는 177억원 (OPM 2.6%), 음 극재는 소폭 적자 (OPM -1%)가 예상된다. 양극재 출하량은 qoq 개선되나 ASP 하락으로 매출액은 전분기 수준으로 예상되고, 음극재는 인조흑연 초기 생산비용 등으로 저조한 수익성이 예상된다. 기초소재는 유가 상승에 따른 화성사업 손익 개선 등으로 안정적인 실적을 기록할 것으로 예상된다.\\n하반기에는 뚜렷한 실적 회복세 예상\\n2Q까지는 yoy 실적 모멘텀이 제한적이나, 하반기에는 뚜렷한 회복세가 예상 된다 (연결OP 3Q 598억원, 4Q 705억원 예상). 리튬 가격 급락이 촉발한 양 극재ASP 하락이 하반기에는 진정될 것으로 예상되기 때문이다. 양극재 가격 은 리튬 가격에 2분기 후행해서 변동되는데, 리튬 가격이 연초에 바닥을 찍 은 후 2~3월 반등했기 때문에 2Q말~3Q부터는 양극재 가격이 안정화될 전 망이다. 하반기 금리 인하까지 현실화된다면, 고객사의 재고 리빌딩과 전기차 구매수요 회복이 맞물려 실적이 빠르게 반등할 수 있을 것으로 예상된다.\\n투자의견 매수, 목표주가 41만원 유지\\n23년 하반기부터 본격화된 양극재 가격 하락은 올해 상반기를 기점으로 마무 리될 전망이다. 더불어 그룹사를 통한 원재료 소싱의 수직계열화는, 하반기부 터 점진적으로 시작돼 2025년에 본격화되며 Peers 대비 양극재 사업의 근원 적 경쟁력이 높아질 전망이다. 실적과 주가에 부담이었던 ASP 하락의 부정적 영향이 마무리되는 시점으로, 매수 접근을 추천한다.\\n투자등급 (기업 투자의견은 향후 6개월간 추천일 종가 대비 해당 종목의 예상수익률을 의미함.)', metadata={'source': 'data\\\\adobe_pdf\\\\json_to_txt\\\\BNK투자증권_포스코퓨처엠_20240409_20240703.txt'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4023a8b-8638-4e19-b396-d5a188503069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter 사용 : 필요에 따라 수정 \n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=10, separators=[' '])\n",
    "#texts = text_splitter.split_documents(documents)\n",
    "\n",
    "#print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c02932-cbf4-484f-bb1d-9f713e5a3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    def __init__(self, stock, keywords, date):\n",
    "        self.stock = stock\n",
    "        self.keywords = keywords\n",
    "        self.date = date\n",
    "\n",
    "    def split_documents(self, documents, delimiters):\n",
    "        split_texts = []\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            pieces, positions = self._split_text(text, delimiters)\n",
    "            for piece, (start, end) in zip(pieces, positions):\n",
    "                if piece.strip():  # 빈 문자열이나 공백만 있는 문자열 제외\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata.update({\n",
    "                        'sdate': self.date,\n",
    "                        'timestamp': self.convert_to_timestamp(self.date),\n",
    "                        'ticket': self.stock,\n",
    "                        'keyword': self.keywords,\n",
    "                        'citation': f\"{{'start': {start}, 'end': {end}}}\", # vectordb 문자열만 인식, 불러올땐 변경 ast 함수 사용  ast.literal_eval()\n",
    "                        'full_text': text\n",
    "                    })\n",
    "                    split_texts.append(Document(page_content=piece, metadata=new_metadata))\n",
    "        return split_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_timestamp(date_str):\n",
    "        datetime_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return int(datetime_object.timestamp())\n",
    "\n",
    "    def _split_text(self, text, delimiters):\n",
    "        pattern = '|'.join(map(re.escape, delimiters))\n",
    "        parts = []\n",
    "        positions = []\n",
    "        start = 0\n",
    "        for m in re.finditer(pattern, text):\n",
    "            end = m.end()\n",
    "            parts.append(text[start:end])  # 구분자 포함하여 추가\n",
    "            positions.append((start, end - 1))\n",
    "            start = end\n",
    "        if start < len(text):\n",
    "            parts.append(text[start:])\n",
    "            positions.append((start, len(text) - 1))\n",
    "        return parts, positions\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "# LineSplitter 클래스 정의\n",
    "class LineSplitter:\n",
    "    def __init__(self, stock, keywords, date):\n",
    "        self.stock = stock\n",
    "        self.keywords = keywords\n",
    "        self.date = date\n",
    "\n",
    "    def split_documents(self, documents):\n",
    "        split_texts = []\n",
    "        for doc_index, doc in enumerate(documents, start=1):\n",
    "            text = doc.page_content\n",
    "            lines = doc.page_content.split('\\n')\n",
    "            for i, line in enumerate(lines, start=1):\n",
    "                new_metadata = doc.metadata.copy()\n",
    "                new_metadata.update({\n",
    "                    'sdate': self.date,  # 날짜 추가\n",
    "                    'timestamp': self.convert_to_timestamp(self.date),  # 날짜 추가\n",
    "                    'ticket': self.stock,  # 종목 추가\n",
    "                    'keyword': self.keywords,  # 키워드 추가\n",
    "                    'citation': f\"line_{i}\",  # 단락 위치 추가\n",
    "                    'full_text': text\n",
    "                })\n",
    "                split_texts.append(Document(page_content=line, metadata=new_metadata))\n",
    "        return split_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_timestamp(date_str):\n",
    "        datetime_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return int(datetime_object.timestamp())\n",
    "\n",
    "    @staticmethod\n",
    "    def timestamp_convert(documents):\n",
    "        key_add = 'timestamp'\n",
    "        for doc in documents:\n",
    "            datetime_object = doc.metadata['sdate']\n",
    "            if isinstance(datetime_object, str):\n",
    "                datetime_object = datetime.strptime(datetime_object, \"%Y-%m-%d\")\n",
    "            timestamp = int(datetime_object.timestamp())\n",
    "            doc.metadata[key_add] = timestamp\n",
    "            doc.metadata['sdate'] = datetime_object.strftime(\"%Y-%m-%d\")  # datetime 객체를 str로 변환\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_documents_by_date_range(documents, start_timestamp, end_timestamp):\n",
    "        filtered_docs = []\n",
    "        for doc in documents:\n",
    "            doc_timestamp = doc.metadata.get('timestamp')\n",
    "            if doc_timestamp and start_timestamp <= doc_timestamp <= end_timestamp:\n",
    "                filtered_docs.append(doc)\n",
    "        return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3d7dd2-214c-4105-9136-c864c9eaf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"포스코퓨처엠\"  # 예시 종목명\n",
    "keywords = \"증권사 리포트\"  # 예시 키워드\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")  # 현재 날짜\n",
    "\n",
    "\n",
    "select = 'text' # 'line'\n",
    "\n",
    "if select == 'text':\n",
    "    # Splitter 인스턴스 생성\n",
    "    splitter = TextSplitter(stock=stock_name, keywords=keywords, date=current_date)\n",
    "\n",
    "    # 구분자 목록\n",
    "    delimiters = ['. '] #, '\\n']\n",
    "    texts = splitter.split_documents(documents, delimiters)\n",
    "\n",
    "if select == 'line':\n",
    "    splitter = LineSplitter(stock=stock_name, keywords=keywords, date=current_date)\n",
    "    # 문서 분할 실행\n",
    "    texts = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025166fd-6b1f-404a-b7f1-d92818d95f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하반기에는 실적 개선 + 원재료 수직계열화 시작\\n재고평가손 환입이 더해져 1Q 실적은 예상보다 양호할 전망 1Q 연결OP 355억원으로 예상보다 양호할 전망이다. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7c27c7-67a4-4f92-b032-5c0fb05bb040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data\\\\adobe_pdf\\\\json_to_txt\\\\BNK투자증권_포스코퓨처엠_20240409_20240703.txt',\n",
       " 'sdate': '2024-07-08',\n",
       " 'timestamp': 1720364400,\n",
       " 'ticket': '포스코퓨처엠',\n",
       " 'keyword': '증권사 리포트',\n",
       " 'citation': \"{'start': 0, 'end': 90}\",\n",
       " 'full_text': '하반기에는 실적 개선 + 원재료 수직계열화 시작\\n재고평가손 환입이 더해져 1Q 실적은 예상보다 양호할 전망 1Q 연결OP 355억원으로 예상보다 양호할 전망이다. 시장 성장 둔화와 ASP 하락으로 에너지소재의 본원 수익성은 낮을 것으로 예상되나, 23.4Q에 인식 된 양극재 재고평가손실 중 일부분이 환입되면서 헤드라인 손익은 예상을 상 회할 전망이다. 손실환입까지 반영한 양극재OP는 177억원 (OPM 2.6%), 음 극재는 소폭 적자 (OPM -1%)가 예상된다. 양극재 출하량은 qoq 개선되나 ASP 하락으로 매출액은 전분기 수준으로 예상되고, 음극재는 인조흑연 초기 생산비용 등으로 저조한 수익성이 예상된다. 기초소재는 유가 상승에 따른 화성사업 손익 개선 등으로 안정적인 실적을 기록할 것으로 예상된다.\\n하반기에는 뚜렷한 실적 회복세 예상\\n2Q까지는 yoy 실적 모멘텀이 제한적이나, 하반기에는 뚜렷한 회복세가 예상 된다 (연결OP 3Q 598억원, 4Q 705억원 예상). 리튬 가격 급락이 촉발한 양 극재ASP 하락이 하반기에는 진정될 것으로 예상되기 때문이다. 양극재 가격 은 리튬 가격에 2분기 후행해서 변동되는데, 리튬 가격이 연초에 바닥을 찍 은 후 2~3월 반등했기 때문에 2Q말~3Q부터는 양극재 가격이 안정화될 전 망이다. 하반기 금리 인하까지 현실화된다면, 고객사의 재고 리빌딩과 전기차 구매수요 회복이 맞물려 실적이 빠르게 반등할 수 있을 것으로 예상된다.\\n투자의견 매수, 목표주가 41만원 유지\\n23년 하반기부터 본격화된 양극재 가격 하락은 올해 상반기를 기점으로 마무 리될 전망이다. 더불어 그룹사를 통한 원재료 소싱의 수직계열화는, 하반기부 터 점진적으로 시작돼 2025년에 본격화되며 Peers 대비 양극재 사업의 근원 적 경쟁력이 높아질 전망이다. 실적과 주가에 부담이었던 ASP 하락의 부정적 영향이 마무리되는 시점으로, 매수 접근을 추천한다.\\n투자등급 (기업 투자의견은 향후 6개월간 추천일 종가 대비 해당 종목의 예상수익률을 의미함.)'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ba4d8f-109f-4f2c-880a-dfe5b5da8671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'start': 0, 'end': 90}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].metadata['citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e2abff6-9af0-4559-84a0-2a8011b2dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'end': 90}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'하반기에는 실적 개선 + 원재료 수직계열화 시작\\n재고평가손 환입이 더해져 1Q 실적은 예상보다 양호할 전망 1Q 연결OP 355억원으로 예상보다 양호할 전망이다. '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorDB 딕셔너리 구조지원안하므로 위와 같이 처리 \n",
    "import ast\n",
    "\n",
    "text_len = texts[0].metadata['citation']\n",
    "\n",
    "print(ast.literal_eval(text_len))\n",
    "start = ast.literal_eval(text_len)['start']\n",
    "end = ast.literal_eval(text_len)['end']\n",
    "# citation 용 \n",
    "texts[0].metadata['full_text'][start:end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65018ece-f894-468e-b69b-aa120464a4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d072640-a099-4ca4-ac77-5229f94f1d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a758b6-baf0-4e3f-a56d-8a842f806097",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 컬렉션 파트 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953b8857-ccb7-47a5-a078-1277d5766b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4bdb78-b1c6-4b89-b81f-3b1d68060df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'sample_vdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5977eebf-0222-4fae-9f70-2d94b3bc6448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=sample_vdb)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections = client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e02421-4a13-4c66-af10-66b7f678639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    name= collection_name,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",                # 코사인 유사도\n",
    "        \"hnsw:construction_ef\": 100,           # HNSW 그래프에서 탐색할 이웃 수 (건설 시)\n",
    "        \"hnsw:M\": 16,                          # 최대 이웃 연결 수\n",
    "        \"hnsw:search_ef\": 10,                 # 검색할 때 탐색할 이웃 수\n",
    "        \"hnsw:num_threads\": 1,                 # 사용할 스레드 수\n",
    "        \"hnsw:resize_factor\": 1.2,               # 그래프 성장률\n",
    "        \"hnsw:batch_size\": 100,                # 브루트포스 인덱스 크기\n",
    "        \"hnsw:sync_threshold\": 1024            # 디스크에 기록할 임계값\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7b50cf-8d66-4e28-a6a8-c9a7bf9bd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제 : 필요시 사용 \n",
    "client.delete_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e396cb-70a9-4703-875c-8da2f3719bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9c374b9-05b8-4ee1-9618-b12dfeec9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 14.013217687606812 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "documents = texts\n",
    "\n",
    "ids = [str(i) for i in range(len(documents))]\n",
    "page_contents = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "embeddings = embedding_function(page_contents) \n",
    "\n",
    "# 실행할 코드 블록 끝 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 실행 시간 계산 및 출력\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "356ebf6f-b730-4293-859c-2b0aa34d42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas, documents=page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d5f59-4837-4056-a02a-042b5e006cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87ff8f-70d6-493f-9bf1-1f902c9a6b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be9d93-0ebf-4ac6-ab01-3dac83551270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d49384d-ef8d-4e95-861d-6e8214d0d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name= collection_name, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a52f2c-3904-422f-95b3-91eb3022294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9db4b5-8f5d-4589-a883-c0cb5dbc1f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'N으로 시작하는 제품명이 뭐지?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단락 테스트 \n",
    "query_text = 'N으로 시작하는 제품명이 뭐지?'\n",
    "query_embedding = embedding_function(query_text)[0] # vdb 불러올때 임베딩 함수 지정안하면 사용\n",
    "print(len(query_embedding))\n",
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285bd8a-2a08-4de4-aaf4-f4427bb58f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efec906-aeb9-48a4-af08-52ffb005e18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N86 제품의 앞으로의 전망은 어떠한가?',\n",
       " '양극재 사업 수익성은 전망이 어떠한가?',\n",
       " '음극재 사업 수익성은 전망이 어떠한가?',\n",
       " '양극재 가격이 안정화될 것이라면 그 근거는 무엇인가?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read the contents of the text file located at './data/query/질문.txt' and extract its contents into a list, with each line as a separate list element.\n",
    "\n",
    "# Define the file path\n",
    "file_path = './data/query/검색해야할질문리스트.txt'\n",
    "\n",
    "# Read the file and extract lines into a list\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Strip newline characters from each line\n",
    "lines = [line.strip() for line in lines]\n",
    "\n",
    "query_list = lines[:-1]\n",
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5531da4e-30bf-49af-a143-2a390029cfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'양극재 가격이 안정화될 것이라면 그 근거는 무엇인가?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = query_list[3]\n",
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4642d883-092d-496d-9d22-8f618658f8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720191600, 1720537200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜 검색용 \n",
    "start_date = datetime(2024, 7, 6)\n",
    "end_date = datetime(2024, 7, 10)\n",
    "\n",
    "start_timestamp = int(start_date.timestamp())\n",
    "end_timestamp = int(end_date.timestamp())\n",
    "\n",
    "start_timestamp, end_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d222b0eb-3da1-4847-be86-35ffb997e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    #query_embeddings=[query_embedding],\n",
    "    query_texts=[query_text],\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"timestamp\": {\"$gt\": start_timestamp}},\n",
    "            {\"timestamp\": {\"$lt\": end_timestamp}},\n",
    "            {\"ticket\": \"포스코퓨처엠\"}\n",
    "        ]\n",
    "    },\n",
    "    n_results=100\n",
    ")\n",
    "\n",
    "#doc_n = list(set(results['documents'][0]))\n",
    "doc_n = results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bb3c34-2112-4fb3-bd65-a7c94f975330",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_n = list(set(results['documents'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e736d334-e563-4fcf-ae00-32f3926adaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranking\n",
    "sent_list = reranker_model(query_text, doc_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc181102-911f-489c-a267-f09f4609d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마지막 질문에 답하려면 다음 문맥을 사용하세요. 답을 모르면 모른다고 말하고 답을 만들어내려고 하지 마세요.\n",
      "당신은 투자 전문가입니다. 다음 보고서 내용의 일부인 문맥 3개를 기반으로 질문에 성실히 답변해주세요.\n",
      "\n",
      "## 문맥 : \n",
      "양극재 가격 은 리튬 가격에 2분기 후행해서 변동되는데, 리튬 가격이 연초에 바닥을 찍 은 후 2~3월 반등했기 때문에 2Q말~3Q부터는 양극재 가격이 안정화될 전 망이다. \n",
      "\n",
      "리튬 가격 급락이 촉발한 양 극재ASP 하락이 하반기에는 진정될 것으로 예상되기 때문이다. \n",
      "\n",
      "최근 메탈 가격 하락에 따른 중장기 추정 양극재 ASP 하향 및 실적 변경에 따라 목표주가 조정\n",
      "4분기까지 이어질 단기 실적 부진이 아쉬우나, 중장기 공급계약으로 바인딩된 물 량 비중이 높은 24년부터 안정적인 실적 성장 가능할 것으로 예상(누적수주합산 얼티엄셀향 35조, SDI향 40조, LGES향 30조). \n",
      "\n",
      "질문: 양극재 가격이 안정화될 것이라면 그 근거는 무엇인가?\n",
      "도움이 되는 답변:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"마지막 질문에 답하려면 다음 문맥을 사용하세요. 답을 모르면 모른다고 말하고 답을 만들어내려고 하지 마세요.\n",
    "당신은 투자 전문가입니다. 다음 보고서 내용의 일부인 문맥 3개를 기반으로 질문에 성실히 답변해주세요.\n",
    "\n",
    "## 문맥 : \n",
    "{doc_1}\n",
    "\n",
    "{doc_2}\n",
    "\n",
    "{doc_3}\n",
    "\n",
    "질문: {user_query}\n",
    "도움이 되는 답변:\"\"\"\n",
    "\n",
    "\n",
    "doc_1 = sent_list[0]\n",
    "doc_2 = sent_list[1]\n",
    "doc_3 = sent_list[2]\n",
    "\n",
    "in_prompt = prompt_template.format(user_query = query_text, doc_1 = doc_1, doc_2 = doc_2,  doc_3 = doc_3 )\n",
    "print(in_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0176e4f-079c-4cae-bcd7-91b76f03d5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f9400e-e488-4320-bfe8-529e0c7fff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AI bot msg] =  양극재 가격이 안정화될 것이라면 그 근거는 리튬 가격이 연초에 바닥을 찍은 후 2~3월에 반등했기 때문에 2Q말~3Q부터는 양극재 가격이 안정화될 전망입니다. 리튬 가격 급락이 촉발한 양극재 ASP 하락이 하반기에 진정될 것으로 예상되기 때문입니다.\n",
      "\n",
      "이 답변은 다음 증권사 리포트 파일을 인용해서 가져왔습니다 :\n",
      "- BNK투자증권_포스코퓨처엠_20240409_20240703.txt 파일의 본문 길이 : 548에서 644까지 인용\n",
      "- BNK투자증권_포스코퓨처엠_20240409_20240703.txt 파일의 본문 길이 : 497에서 547까지 인용\n",
      "- 포스코퓨처엠_대신증권_20231025_20240703.txt 파일의 본문 길이 : 203에서 377까지 인용\n",
      "time : 47.9048056602478\n"
     ]
    }
   ],
   "source": [
    "## 여기서 부터 추론 \n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# transformers의 로깅 설정을 조정하여 경고 메시지 억제\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "sys_msg = \"당신은 인공지능 어시스턴트입니다. 묻는 말에 친절하고 정확하게 답변하세요.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_msg},\n",
    "    ]\n",
    "\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "#user_input = input('[user msg] = ')\n",
    "user_input = in_prompt\n",
    "\n",
    "user_msg = {\"role\": \"user\", \"content\": user_input}\n",
    "messages.append(user_msg)\n",
    "\n",
    "chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "prompt = chat\n",
    "\n",
    "response = inference_output(prompt)\n",
    "split_output = '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "outputs = response.split(split_output)[-1]\n",
    "try:\n",
    "    end_token = '<|end_of_text|'\n",
    "    outputs = outputs.replace(end_token)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "citation_prompt =  citation_extract_prompt_add_v000(results, doc_1, doc_2, doc_3)\n",
    "\n",
    "outputs_ex = outputs + '\\n\\n' + citation_prompt\n",
    "\n",
    "print('[AI bot msg] = ', outputs_ex) # outputs_ex\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34815f22-9abe-44e2-9d4e-8bd781168f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36b45b1e-8742-411f-9d63-93a6c2afbe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 모델과 토크나이저 언로드\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# 메모리 정리\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d260f2-a089-4299-9185-af994e7c4120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 723 tensors from ./models/kiqu-70b.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32764\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q2_K:  321 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q5_K:   80 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32764\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32764\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 68.98 B\n",
      "llm_load_print_meta: model size       = 23.71 GiB (2.95 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    1.84 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    82.03 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  6298.69 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  5998.75 MiB\n",
      "llm_load_tensors:      CUDA2 buffer size =  5998.75 MiB\n",
      "llm_load_tensors:      CUDA3 buffer size =  5903.92 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   336.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_init:      CUDA2 KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_init:      CUDA3 KV buffer size =   304.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   672.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   672.01 MiB\n",
      "llama_new_context_with_model:      CUDA2 compute buffer size =   672.01 MiB\n",
      "llama_new_context_with_model:      CUDA3 compute buffer size =   672.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    48.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 5\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'models', 'general.architecture': 'llama', 'llama.context_length': '32764', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '8192', 'llama.block_count': '80', 'llama.feed_forward_length': '28672', 'llama.attention.head_count': '64', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + '\\n' + '[/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + '\\n\\n' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + '\n",
      "' + '[/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + '\n",
      "\n",
      "' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "## 번외 : 양자화 모델을 사용시 \n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#model_name = \"./models/EEVE-Korean-Instruct-10.8B-q4_0.gguf\"\n",
    "model_name = \"./models/kiqu-70b.Q2_K.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=model_name,\n",
    "      n_gpu_layers= 81, # Uncomment to use GPU acceleration   -1, 0~  \n",
    "      n_gpu=4,  # 사용할 GPU의 수 설정 (여기서는 4개)\n",
    "      #seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=4096, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ceaae3e-9c28-4a13-8d00-572749895e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너는 kiqu-70B라는 한국어에 특화된 언어모델이야. 깔끔하고 자연스럽게 대답해줘!\n",
      "[INST] 마지막 질문에 답하려면 다음 문맥을 사용하세요. 답을 모르면 모른다고 말하고 답을 만들어내려고 하지 마세요.\n",
      "당신은 투자 전문가입니다. 다음 보고서 내용의 일부인 문맥 3개를 기반으로 질문에 성실히 답변해주세요.\n",
      "\n",
      "## 문맥 : \n",
      "양극재 가격 은 리튬 가격에 2분기 후행해서 변동되는데, 리튬 가격이 연초에 바닥을 찍 은 후 2~3월 반등했기 때문에 2Q말~3Q부터는 양극재 가격이 안정화될 전 망이다. \n",
      "\n",
      "리튬 가격 급락이 촉발한 양 극재ASP 하락이 하반기에는 진정될 것으로 예상되기 때문이다. \n",
      "\n",
      "최근 메탈 가격 하락에 따른 중장기 추정 양극재 ASP 하향 및 실적 변경에 따라 목표주가 조정\n",
      "4분기까지 이어질 단기 실적 부진이 아쉬우나, 중장기 공급계약으로 바인딩된 물 량 비중이 높은 24년부터 안정적인 실적 성장 가능할 것으로 예상(누적수주합산 얼티엄셀향 35조, SDI향 40조, LGES향 30조). \n",
      "\n",
      "질문: 양극재 가격이 안정화될 것이라면 그 근거는 무엇인가?\n",
      "도움이 되는 답변:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prompt_template = \"\"\"너는 kiqu-70B라는 한국어에 특화된 언어모델이야. 깔끔하고 자연스럽게 대답해줘!\n",
    "[INST] 안녕?\n",
    "[/INST] 안녕하세요! 무엇을 도와드릴까요? 질문이나 궁금한 점이 있다면 언제든지 말씀해주세요.\n",
    "\n",
    "[INST] {instruction}\n",
    "[/INST]\"\"\"\n",
    "'''\n",
    "\n",
    "prompt_template = \"\"\"너는 kiqu-70B라는 한국어에 특화된 언어모델이야. 깔끔하고 자연스럽게 대답해줘!\n",
    "[INST] {instruction}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "\n",
    "#instruction = input('user query =')#\"안녕?\"\n",
    "instruction = in_prompt\n",
    "\n",
    "prompt = prompt_template.format(instruction = instruction)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573f7699-1863-4b22-b9df-33dc3c65b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7533.93 ms\n",
      "llama_print_timings:      sample time =     248.10 ms /  1247 runs   (    0.20 ms per token,  5026.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9401.72 ms /   831 tokens (   11.31 ms per token,    88.39 tokens per second)\n",
      "llama_print_timings:        eval time =  406446.93 ms /  1246 runs   (  326.20 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =  422682.92 ms /  2077 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'id': 'cmpl-d4f35172-4be7-4710-9da3-1f75cc6dcbc4', 'object': 'text_completion', 'created': 1720424863, 'model': './models/kiqu-70b.Q2_K.gguf', 'choices': [{'text': ' 양극재 가격의 안정화에 대한 근거는, 리튬 가격의 2~3월에 이른 반등과 관련이 있어요. 리튬 가격이 연초에 바닥을 찍은 후 상승하면서, 양극재 가격 또한 2Q말~3Q부터는 안정화될 전망이 열려요. 이 지출은, 리튬 가격의 상승에 2분기 뒤로 미루어진 변동 특성과 리튬 가격 하락이 촉발한 양 극재ASP의 하락 정도를 감안하메로, 중장기 공급 계약을 통해 유지되는 상태에서 실적의 안정성을 보여주고 있어요. 따라서, 리튬 가격의 변동과 연결된 양극재ASP의 하락이 중단될 것으로 보인다면, 이는 양극재 가격 안정화에 대한 중요한 근거가 될 수 있어요.\\n\\n이와 관련해서, 최근 메탈 가격 하락과 연계된 중장기 추정 양극재 ASP의 하향 조정도 고려해야 할 요소일 수 있어요. 이러한 상황을 통해, 실질적인 가격 안정화의 여지는 중장기 공급 계약과 관련 업체들의 상활을 포착하는 것이 중요할 거예요. \\n\\n결국, 리튬 가격의 변동과 연관지어진 양 극재ASP의 하락이 중단될 것으로 보인다면, 이는 양극재 가격 안정화에 대한 강력한 근거가 되어 할 거예요. 하지만, 중장기 공급 계약과 관련 업체들의 상황을 포착할 가능성도 고려해야 하며, 추후에 발생할 수 있는 다른 관련 요소들이 또한 중요한 근거가 될 수 있어요. \\n\\n양극재 가격의 안정화에 대한 이해를 위해서는, 리튬 가격의 변동과 연관지은 양 극재ASP의 하락 정도, 중장기 공급 계약을 통한 유지되는 상태, 그리고 최근 메탈 가격의 하락과 이에 따른 중장기 추정 양극재 ASP 하향 조정 등을 종합해 보는 것이 필요해요. 이러한 복합적인 요소들을 고려하면, 양극재 가격의 안정화에 대한 근거를 더 명확하게 이해할 수 있을 거예요.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 831, 'completion_tokens': 1246, 'total_tokens': 2077}}\n",
      "\n",
      "Execution Time: 422.6915271282196 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# 입력 텍스트 설정\n",
    "#prompt = \"Hello, how are you?\"\n",
    "\n",
    "# 모델 실행\n",
    "output = llm(\n",
    "      prompt, # Prompt\n",
    "      max_tokens= 4096, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      stop=['</s>'], #, '## summary', '\\n\\n'],\n",
    "      #top_k = 30 ,\n",
    "      top_p= 0.92 ,\n",
    "      temperature= 0.85,\n",
    "      echo= False # Echo the prompt back in the output\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Output:\", output)\n",
    "print()\n",
    "# 실행할 코드 블록 끝 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 실행 시간 계산 및 출력\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d830fc3f-39cc-4e33-b6ee-9142415ae02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양극재 가격의 안정화에 대한 근거는, 리튬 가격의 2~3월에 이른 반등과 관련이 있어요. 리튬 가격이 연초에 바닥을 찍은 후 상승하면서, 양극재 가격 또한 2Q말~3Q부터는 안정화될 전망이 열려요. 이 지출은, 리튬 가격의 상승에 2분기 뒤로 미루어진 변동 특성과 리튬 가격 하락이 촉발한 양 극재ASP의 하락 정도를 감안하메로, 중장기 공급 계약을 통해 유지되는 상태에서 실적의 안정성을 보여주고 있어요. 따라서, 리튬 가격의 변동과 연결된 양극재ASP의 하락이 중단될 것으로 보인다면, 이는 양극재 가격 안정화에 대한 중요한 근거가 될 수 있어요.\n",
      "\n",
      "이와 관련해서, 최근 메탈 가격 하락과 연계된 중장기 추정 양극재 ASP의 하향 조정도 고려해야 할 요소일 수 있어요. 이러한 상황을 통해, 실질적인 가격 안정화의 여지는 중장기 공급 계약과 관련 업체들의 상활을 포착하는 것이 중요할 거예요. \n",
      "\n",
      "결국, 리튬 가격의 변동과 연관지어진 양 극재ASP의 하락이 중단될 것으로 보인다면, 이는 양극재 가격 안정화에 대한 강력한 근거가 되어 할 거예요. 하지만, 중장기 공급 계약과 관련 업체들의 상황을 포착할 가능성도 고려해야 하며, 추후에 발생할 수 있는 다른 관련 요소들이 또한 중요한 근거가 될 수 있어요. \n",
      "\n",
      "양극재 가격의 안정화에 대한 이해를 위해서는, 리튬 가격의 변동과 연관지은 양 극재ASP의 하락 정도, 중장기 공급 계약을 통한 유지되는 상태, 그리고 최근 메탈 가격의 하락과 이에 따른 중장기 추정 양극재 ASP 하향 조정 등을 종합해 보는 것이 필요해요. 이러한 복합적인 요소들을 고려하면, 양극재 가격의 안정화에 대한 근거를 더 명확하게 이해할 수 있을 거예요.\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b54ef-ad1e-4895-8c36-35a400c902a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aba909-e29e-4aae-914c-58ae06899967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75821142-4a7b-4560-b177-03c1077c0bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc85d11a-35d2-45ae-a9c4-fdb47b0b7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_output(prompt):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False, # 올거나이저 지침사항\n",
    "        #top_p= 0.92,# 누적 확률을 기준으로 역순으로 단어를 정렬, 지정한 값에 도달하는 순간 멈춤 (단어 선별, 확률 분포의 긴꼬리를 자름 -> 자연스러운 텍스트 생성)\n",
    "        #top_k=20, # 특별한 이유없으면 1로 지정 , top_p와 다르게 누적 건수를 기준으로 선별 \n",
    "        #no_repeat_ngram_size=3,\n",
    "        #temperature= 0.85, # 0.37,\n",
    "        early_stopping= True,\n",
    "        #eos_token_id= terminators,  # 종료 토큰 지정 : 2 = </s> ,   46332 =  <|endoftext|>\n",
    "        ##pad_token_id= tokenizer.eos_token_id,\n",
    "        #eos_token_id= tokenizer.eos_token_id,\n",
    "        num_beams=3,\n",
    "        repetition_penalty=1.05,\n",
    "\n",
    "        \n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f359df3-5617-47b1-8636-465839806880",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 벡터 임베딩 : gpt (필요시 사용, 본 스터디에서는 로컬이 목적)\n",
    "\"\"\"\n",
    "class MyOpenAIEmbeddingFunction:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, str):\n",
    "            input = [input]  # Ensure the input is in list form\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            # API call to get embeddings for each text in the list\n",
    "            response = openai.Embedding.create(\n",
    "                model=self.model_name,\n",
    "                input=text\n",
    "            )\n",
    "            # Extract and append the embedding from the response\n",
    "            embeddings.append(response['data'][0]['embedding'])\n",
    "        \n",
    "        return embeddings\n",
    "# Instantiate the embedding function\n",
    "embedding_function = MyOpenAIEmbeddingFunction()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b000b0e-0422-4ca9-a544-4c484fd5ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560c9670c14b413685aca4131e74bd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 4*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "## 벡터 임베딩 \n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "\n",
    "class MyEmbeddingFunction:\n",
    "    def __init__(self):\n",
    "        self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        # Encode the input text\n",
    "        embeddings = self.model.encode(input, \n",
    "                                       batch_size=12, \n",
    "                                       max_length= 4096)['dense_vecs']\n",
    "        return embeddings\n",
    "\n",
    "# Instantiate the embedding function\n",
    "embedding_function = MyEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810d99e5-1b41-4789-a591-75df5c1730ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f57023cd7e74872a9f452bcc5324853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## 로컬 모델\n",
    "### 실험 결과, 금융 로컬모델 중 뛰어난 성능 : 표 데이터 인식 등 \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "#torch.set_default_device('cuda')\n",
    "device_map = 'auto'\n",
    "\n",
    "model_id  = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    #low_cpu_mem_usage=True,\n",
    "    #return_dict=True,\n",
    "    torch_dtype= \"auto\", # torch.float16,\n",
    "    device_map=device_map,)\n",
    "\n",
    "#model.eval()\n",
    "#model.config.use_cache = (True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb16ad3-6618-4cb4-9a3a-787f5c855a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def citation_extarct(results, doc_n):\n",
    "    check_list = results['documents'][0]\n",
    "    metas_list = results['metadatas'][0]\n",
    "\n",
    "    ok_list = []\n",
    "\n",
    "    for r in range(len(check_list)):\n",
    "        snippet = check_list[r]\n",
    "        if doc_n in snippet:\n",
    "            cit = metas_list[r]['citation']\n",
    "            p_p = metas_list[r]['source']\n",
    "        \n",
    "            ok_list.append([cit, p_p])\n",
    "    return ok_list\n",
    "\n",
    "def citation_extract_prompt_add(results, doc_1, doc_2, doc_3):\n",
    "    line_1, cp_path_1 = citation_extarct(results, doc_1)[0]\n",
    "    line_2, cp_path_2 = citation_extarct(results, doc_2)[0]\n",
    "    line_3, cp_path_3 = citation_extarct(results, doc_3)[0]\n",
    "\n",
    "    citation_template = \"\"\"이 답변은 다음 증권사 리포트 파일을 인용해서 가져왔습니다 :\n",
    "- {cp_path_1}파일의 {line_1}에서 인용\n",
    "- {cp_path_2}파일의 {line_2}에서 인용\n",
    "- {cp_path_3}파일의 {line_3}에서 인용\"\"\"\n",
    "\n",
    "    citation_prompt = citation_template.format(cp_path_1 = cp_path_1 , cp_path_2 = cp_path_2, cp_path_3 = cp_path_3 ,\n",
    "                                          line_1 = line_1 , line_2 =  line_2, line_3 = line_3)\n",
    "\n",
    "    return citation_prompt\n",
    "\n",
    "\n",
    "# 임의로 만든 인용 추출기 : 구버전 , 함수에서 _v000제거할것\n",
    "def citation_extract_prompt_add_v000(results, doc_1, doc_2, doc_3): \n",
    "    doc_1_c = citation_extarct(results, doc_1)\n",
    "    doc_2_c =citation_extarct(results, doc_2)\n",
    "    doc_3_c = citation_extarct(results, doc_3)\n",
    "\n",
    "    cp_path_1, sp_1, ep_1 = doc_citation_extract(doc_1_c)\n",
    "    cp_path_2, sp_2, ep_2 = doc_citation_extract(doc_2_c)\n",
    "    cp_path_3, sp_3, ep_3 = doc_citation_extract(doc_3_c)\n",
    "\n",
    "\n",
    "    citation_template = \"\"\"이 답변은 다음 증권사 리포트 파일을 인용해서 가져왔습니다 :\n",
    "- {citation_pdf_path_1} 파일의 본문 길이 : {start_page_1}에서 {end_page_1}까지 인용\n",
    "- {citation_pdf_path_2} 파일의 본문 길이 : {start_page_2}에서 {end_page_2}까지 인용\n",
    "- {citation_pdf_path_3} 파일의 본문 길이 : {start_page_3}에서 {end_page_3}까지 인용\"\"\"\n",
    "\n",
    "    citation_prompt = citation_template.format(citation_pdf_path_1 = cp_path_1 , citation_pdf_path_2 = cp_path_2, citation_pdf_path_3 = cp_path_3 ,\n",
    "                                          start_page_1 = sp_1 , start_page_2 =  sp_2, start_page_3 = sp_3 ,\n",
    "                                          end_page_1 = ep_1 , end_page_2 = ep_2, end_page_3 = ep_3 )\n",
    "    return citation_prompt\n",
    "\n",
    "def citation_extarct_v000(results, doc_n):\n",
    "    check_list = results['documents'][0]\n",
    "    metas_list = results['metadatas'][0]\n",
    "\n",
    "    ok_list = []\n",
    "\n",
    "    for r in range(len(check_list)):\n",
    "        snippet = check_list[r]\n",
    "        if doc_n in snippet:\n",
    "            cit = metas_list[r]['citation']\n",
    "            p_p = metas_list[r]['pdf']\n",
    "        \n",
    "            ok_list.append([cit, p_p])\n",
    "    return ok_list\n",
    "\n",
    "def doc_citation_extract(doc_n_c):\n",
    "    dict_str = doc_n_c[0][0]\n",
    "    # ast.literal_eval을 사용하여 문자열을 딕셔너리로 변환\n",
    "    dict_obj = ast.literal_eval(dict_str)\n",
    "    dict_obj['start'], dict_obj['end']\n",
    "\n",
    "    citation_pdf_path = doc_n_c[0][1].split('\\\\')[-1] # 경로 \n",
    "\n",
    "    return citation_pdf_path, dict_obj['start'], dict_obj['end']\n",
    "\n",
    "def inference_output(prompt):\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    #input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    # 인퍼런스 커스텀\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\") # return_token_type_ids=False\n",
    "    # peft 일시, **tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False) 아래 inputs['input_ids'] 교체 , 위는 삭제 \n",
    "    output = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False),\n",
    "        max_new_tokens = 1024, # 200  256 512\n",
    "        do_sample= True, # temperature 매개변수는 top_p 및 top_k와 함께 do_sample=True일 때만 활성화\n",
    "        top_p= 0.92,# 누적 확률을 기준으로 역순으로 단어를 정렬, 지정한 값에 도달하는 순간 멈춤 (단어 선별, 확률 분포의 긴꼬리를 자름 -> 자연스러운 텍스트 생성)\n",
    "        top_k=20, # 특별한 이유없으면 1로 지정 , top_p와 다르게 누적 건수를 기준으로 선별 \n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature= 0.3, # 0.37,\n",
    "        early_stopping= True,\n",
    "        eos_token_id=terminators,  # 종료 토큰 지정 : 2 = </s> ,   46332 =  <|endoftext|>\n",
    "        repetition_penalty=1.2,\n",
    "        #pad_token_id= tokenizer.eos_token_id,\n",
    "        #eos_token_id= tokenizer.eos_token_id,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    output = output[0].to(\"cpu\")\n",
    "    return tokenizer.decode(output)\n",
    "\n",
    "# Document 클래스 정의\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
    "\n",
    "# 텍스트를 일정한 크기로 나누고 오버랩하는 클래스\n",
    "class CharacterTextSplitter:\n",
    "    def __init__(self, chunk_size, chunk_overlap, separator=''):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separator = separator\n",
    "\n",
    "    def split_text(self, text):\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            if self.separator:\n",
    "                end = text.find(self.separator, start, end)\n",
    "                if end == -1:\n",
    "                    end = min(start + self.chunk_size, len(text))\n",
    "                else:\n",
    "                    end += len(self.separator)\n",
    "            chunk = text[start:end]\n",
    "            chunks.append((chunk, start, end - 1))\n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "    def split_documents(self, documents):\n",
    "        split_docs = []\n",
    "        for doc in documents:\n",
    "            text_chunks = self.split_text(doc.page_content)\n",
    "            for chunk, start_p, end_p in text_chunks:\n",
    "                # 메타데이터에 citation 정보 추가\n",
    "                metadata_with_citation = {**doc.metadata, 'citation': {'start_p': start_p, 'end_p': end_p}}\n",
    "                split_docs.append(Document(page_content=chunk, metadata=metadata_with_citation))\n",
    "        return split_docs\n",
    "\n",
    "\n",
    "def timestamp_convert(documents): # datetime string\n",
    "    key_add = 'timestamp'\n",
    "    for doc in documents:\n",
    "        datetime_object = doc.metadata['wdate']\n",
    "        if isinstance(datetime_object, str):\n",
    "            # wdate가 문자열인 경우, datetime 객체로 변환\n",
    "            datetime_object = datetime.strptime(datetime_object, \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp = int(datetime_object.timestamp())\n",
    "        doc.metadata[key_add] = timestamp\n",
    "        doc.metadata['wdate'] = str(datetime_object)  # datetime 객체를 str로 변환\n",
    "    return documents\n",
    "\n",
    "# reranker model load\n",
    "def reranker_model(query_text, doc_n):\n",
    "    def exp_normalize(x):\n",
    "        b = x.max()\n",
    "        y = np.exp(x - b)\n",
    "        return y / y.sum()\n",
    "    \n",
    "    rmodel_path = 'Dongjin-kr/ko-reranker'\n",
    "\n",
    "    rtokenizer = AutoTokenizer.from_pretrained(rmodel_path)\n",
    "    rmodel = AutoModelForSequenceClassification.from_pretrained(rmodel_path)\n",
    "    rmodel.eval()\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(doc_n)):\n",
    "        docs = doc_n[i]\n",
    "        query_doc = [query_text, docs ]\n",
    "        pairs.append(query_doc)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rtokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = rmodel(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        scores = exp_normalize(scores.numpy())\n",
    "        #print (f'first: {scores[0]}, second: {scores[1]}')\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    sorted_values = scores[sorted_indices]\n",
    "\n",
    "    index_numbers = sorted_indices[:3]\n",
    "\n",
    "    sent_list = []\n",
    "\n",
    "    for d in range(len(index_numbers)):\n",
    "        index_n = index_numbers[d]\n",
    "        sent = doc_n[index_n]\n",
    "        #print(sent)\n",
    "        sent_list.append(sent)\n",
    "    return sent_list\n",
    "\n",
    "## 함수 추가 \n",
    "\n",
    "# 패턴이 동일하면 할 날짜 \n",
    "def file_date_format(file_path):\n",
    "    # 정규 표현식을 사용하여 날짜 추출\n",
    "    date_string = re.search(r'\\d{6}', file_path).group()\n",
    "\n",
    "    # 문자열을 datetime 객체로 변환\n",
    "    date_object = datetime.strptime(date_string, '%Y%m%d')\n",
    "    \n",
    "    return date_object\n",
    "\n",
    "\n",
    "# 표 데이터 제거\n",
    "def remove_table_data(text):\n",
    "    # 숫자와 기호로 이루어진 표 패턴 탐지 및 제거\n",
    "    table_pattern = re.compile(r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?(?:[\\%\\원억십조천백]*)?\\s*)+', re.MULTILINE)\n",
    "    cleaned_text = re.sub(table_pattern, '', text)\n",
    "    \n",
    "    # '표' 또는 'Fig'가 들어간 줄 제거\n",
    "    cleaned_text = re.sub(r'표\\d+.*|Fig\\.\\s*\\d+.*', '', cleaned_text)\n",
    "    \n",
    "    # 추가적으로 다중 공백, 특수 문자 정리\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', cleaned_text)  # 다중 공백을 한 줄로 정리\n",
    "    cleaned_text = re.sub(r'[^\\S\\r\\n]+', ' ', cleaned_text)  # 다중 공백을 한 칸 공백으로 정리\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # 연속된 공백을 한 칸 공백으로 정리\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)  # 연속된 줄 바꿈을 한 줄로 정리\n",
    "    cleaned_text = re.sub(r'[ ]*표[ ]*\\d+', '', cleaned_text)  # '표'와 숫자가 포함된 문자열 제거\n",
    "    cleaned_text = re.sub(r'Fig[ ]*\\d+', '', cleaned_text)  # 'Fig'와 숫자가 포함된 문자열 제거\n",
    "    cleaned_text = remove_specific_pattern(cleaned_text)\n",
    "    \n",
    "\n",
    "# 특정 패턴 제거 함수\n",
    "def remove_specific_pattern(text):\n",
    "    # 텍스트 내의 특정 패턴을 정의하고 제거\n",
    "    specific_pattern = re.compile(r'\\n\\(천 원 \\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\(십 억 원 \\)\\n\\n\\n\\n\\n\\n\\(\\)\\n\\(\\)\\n\\(\\)\\n\\n\\n\\n\\n\\n\\n주\\n\\n\\n\\n가\\n\\.X\\n\\n\\n\\n영\\n\\n\\n\\n업\\n\\n\\n\\n현\\n\\n\\n\\n금\\n\\n\\.X\\n\\.X\\n\\n흐 름\\n\\n\\n\\n\\n\\n\\.X\\n\\.X\\n\\n\\n\\n\\n\\n\\n\\(천 원\\n\\n\\n\\n\\n\\n\\n\\n\\(십 억\\n,\\n,\\n,\\n,\\n\\n\\)\\n\\n원\\n\\n\\n\\n\\n\\n\\n\\n\\)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n주\\n\\n가\\nX\\n\\n\\n\\n\\nX\\nX\\n\\nC a p e x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nX\\nX\\n\\n\\n\\n\\n\\n\\n\\(십억원\\) \\(십억원\\)\\nFree Cash Flow 순차입금\\n\\n\\n\\n\\n\\(\\) \\n\\n\\(\\) \\n\\n\\(\\)\\n\\n\\(\\) \\(\\)\\n\\n재무상태표 포괄손익계산서\\n \\n \\n \\n \\n 률 \\n비 \\n \\n \\n 률 \\n \\n ---\\n ----\\n -\\n비 기타 --\\n -\\n 률 -\\n -\\n \\n \\n \\n 률 \\n \\n -\\n현금흐름표 주요투자지표\\n \\n --\\n \\n \\n \\n -----\\n ----\\n감소 ----\\n감소 -----\\n증가 EV/ \\n -----\\n -----\\n증가 -----증가율 \\n감소 증가율 -\\n순감 ----\\n --\\n \\n \\n -----/자기자본 \\n -\\n \\n ------\\n', re.MULTILINE)\n",
    "    cleaned_text = re.sub(specific_pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "    \n",
    "def clean_text_by_pdf(text):\n",
    "    # 정규식을 이용해 표를 제거\n",
    "    clean_text = re.sub(r\"\\n주.*\\n\", \"\\n\", text)  # '주:'로 시작하는 줄 제거\n",
    "    clean_text = re.sub(r\"\\n\\s*PER.*\\n\", \"\\n\", clean_text)  # 'PER'로 시작하는 줄 제거\n",
    "    clean_text = re.sub(r\"\\d{4} \\d{4} \\d{4} \\d{4}\", \"\", clean_text)  # 연도별 데이터 줄 제거\n",
    "    #clean_text = re.sub(r'(?<=[가-힣])\\\\n(?=[가-힣])', '', clean_text)\n",
    "    #clean_text = re.sub(r'(?<=[가-힣])\\n(?=[가-힣])', '', clean_text)\n",
    "    clean_text = clean_text.replace('\\\\n\\\\n','')\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def prompt_clean_pdf_content(pdf_text, model_name):\n",
    "    p_template = \"\"\"pdf 파일의 내용을 텍스트 추출기를 통해 추출한 내용을 전처리 하십시오. 다음 지침을 충실히 따르십시오.\n",
    "    - \"다음은 테이블과 이미지 관련 내용을 제외한 텍스트 내용입니다:\" 와 같은 서두는 하지말아야 합니다.\n",
    "    - 매출전표, 재무재표와 같은 표로 구성된 텍스트는 제외해주세요.\n",
    "    -  ■■■■■ 이렇게 붙은 텍스트 행도 표로 간주하고 제외해주세요.\n",
    "\n",
    "    ### 삭제해야할 숫자 포함 표 텍스트 행 :\n",
    "    매출액 3,302 4,760 4,619 6,040 8,457 유동자산 2,038 2,412 2,400 3,516 2,825\n",
    "\n",
    "    매출원가 2,967 4,503 4,215 5,527 7,738 현금및현금성자산 281 390 310 1,284 118\n",
    "        \n",
    "    매출총이익 335 257 405 513 719 매출채권 및 기타채권 292 770 778 855 1,069\n",
    "\n",
    "    {pdf_text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = p_template.format(pdf_text = pdf_text)\n",
    "\n",
    "    result = get_completion(prompt, model_name, temperature=0, verbose=False)\n",
    "    return result\n",
    "\n",
    "#pdf_text\n",
    "#text  = clean_text_by_pdf(pdf_text)\n",
    "\n",
    "# 결과 출력\n",
    "#print(text)\n",
    "\n",
    "def prompt_template_news(ticker, title, body):\n",
    "    prompt_template = \"\"\"Based on the following news article, please come up with one of the most important questions that average investors would ask.\n",
    "only answer is query. \n",
    "\n",
    "# news article : \n",
    "- ticker : \n",
    "{ticker}\n",
    "\n",
    "- article title : \n",
    "{title}\n",
    "\n",
    "- article content : \n",
    "{body}\n",
    "\n",
    "\n",
    "# query generate : \"\"\"\n",
    "    prompt_insert = prompt_template.format(ticker = ticker, title = title, body = body)\n",
    "    \n",
    "    return prompt_insert\n",
    "\n",
    "\n",
    "# 텍스트를 임베딩으로 변환하는 함수 정의\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().tolist()\n",
    "\n",
    "# error, retry 추가\n",
    "def get_completion(prompt, model, temperature=0, verbose=False):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    retry_count = 3\n",
    "    for i in range(0, retry_count):\n",
    "        while True:\n",
    "            try:    \n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature, # this is the degree of randomness of the model's output\n",
    "                )\n",
    "                answer = response['choices'][0]['message']['content'].strip()                \n",
    "                tokens = response.usage.total_tokens                \n",
    "                \n",
    "                \n",
    "                time_end = time.time()\n",
    "                \n",
    "                if verbose:\n",
    "                    print('prompt: %s | token: %d | %.1fsec\\nanwer : %s'%(prompt, tokens, (time_end - time_start), answer))\n",
    "                return answer\n",
    "            \n",
    "            except Exception as error:\n",
    "                print(f\"API Error: {error}\")\n",
    "                print(f\"Retrying {i+1} time(s) in 4 seconds...\")\n",
    "                \n",
    "                if i+1 == retry_count:\n",
    "                    return prompt, None, None\n",
    "                time.sleep(4)\n",
    "                continue\n",
    "                \n",
    "def target_date_select(target_date):\n",
    "    if target_date.find(',') >= 0 :\n",
    "        td_list = target_date.split(',')\n",
    "    else:\n",
    "        td_list = [target_date.strip()]\n",
    "    return td_list \n",
    "\n",
    "def target_datetime_extract(date_range_string, data):\n",
    "    date_range_string = date_range_string.strip()\n",
    "    start_date_str, end_date_str = date_range_string.split(\"~\")\n",
    "\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    target_date_data = [\n",
    "        item for item in data if start_date.date() <= item[3].date() < end_date.date()\n",
    "    ]\n",
    "    return target_date_data\n",
    "\n",
    "def oracle_news_load(code):\n",
    "    sql = \"\"\"\n",
    "    SELECT A.SN, TITLE, CONTENT, WDATE, FILENAME FROM\n",
    "\n",
    "    (SELECT * FROM NEWS_VIEW WHERE CODES = '\"\"\"+code+\"\"\"' AND SOURCE ='10' AND  ROWNUM <= 10000) A, \n",
    "\n",
    "    (SELECT * FROM NEWS_CONT)B\n",
    "\n",
    "    WHERE A.SN = B.SN ORDER BY SN DESC \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # 결과 레코드를 차례대로 리스트에 저장\n",
    "    result_list = []\n",
    "    for row in cursor:\n",
    "        result_list.append(row)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "## 문장 분리기 : 31 이하 단어 수는 삭제 (불필요한 짧은 문장 제거)\n",
    "def body_split_token(text):\n",
    "    #kkma = Kkma()\n",
    "    #token_sentence = kkma.sentences(text)\n",
    "    token_sentence =  split_sentences(text)\n",
    "    doc_token_list = []\n",
    "\n",
    "    for t in range(len(token_sentence)): \n",
    "        doc_token = token_sentence[t]\n",
    "        doc_token = re.sub('&nbsp-','', doc_token)\n",
    "        if len(doc_token) <= 31:\n",
    "            continue\n",
    "\n",
    "        doc_token_list.append(doc_token)\n",
    "        \n",
    "    return doc_token_list\n",
    "\n",
    "\n",
    "def oracle_sql_select(code):\n",
    "    sql = \"\"\"\n",
    "    SELECT A.SN, TITLE, CONTENT FROM\n",
    "\n",
    "    (SELECT * FROM NEWS_VIEW WHERE CODES = '\"\"\"+code+\"\"\"' AND SOURCE ='10' AND  ROWNUM <= 1000) A, \n",
    "\n",
    "    (SELECT * FROM NEWS_CONT)B\n",
    "\n",
    "    WHERE A.SN = B.SN ORDER BY SN DESC \n",
    "\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # 결과 레코드를 차례대로 리스트에 저장\n",
    "    result_list = []\n",
    "    for row in cursor:\n",
    "        result_list.append(row)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "def sql_output_data(result_list, code):\n",
    "    news_list = []\n",
    "\n",
    "    for r in range(len(result_list)):\n",
    "        index_number = result_list[r][0]\n",
    "        title = result_list[r][1].strip()\n",
    "        body_origin = result_list[r][2].read()\n",
    "        soup = BeautifulSoup(body_origin, 'html.parser')\n",
    "        text_body = soup.find_all('p')\n",
    "        if len(text_body) <= 0:\n",
    "            body = body_origin\n",
    "        else:\n",
    "            # BeautifulSoup을 사용하여 HTML 파싱\n",
    "            body = body_html_del(body_origin)\n",
    "            # BeautifulSoup을 사용하여 HTML 파싱\n",
    "        http_only_check = http_url_search(body)\n",
    "        if http_only_check == True:  # 링크들만 있으면 스킵\n",
    "            continue\n",
    "        if len(body) ==0:\n",
    "            continue\n",
    "        ndata = [index_number, code, title, body]\n",
    "        news_list.append(ndata)\n",
    "        \n",
    "    return news_list\n",
    "\n",
    "## 함수 만들기 \n",
    "\n",
    "def body_html_del(body_origin):\n",
    "    soup = BeautifulSoup(body_origin, 'html.parser')\n",
    "    text_body = soup.find_all('p')\n",
    "    body_p = ''\n",
    "    for b in range(len(text_body)):\n",
    "        tbody = text_body[b].text\n",
    "        tbody = tbody.replace('\\xa0','')\n",
    "        body_kor_check = contains_korean(tbody)\n",
    "\n",
    "        if body_kor_check == False:  # 링크들만 있으면 스킵\n",
    "            continue\n",
    "        if b == 0:\n",
    "            body_p = tbody\n",
    "        else:\n",
    "            body_p = body_p +' '+ tbody\n",
    "        \n",
    "    return body_p\n",
    "\n",
    "# 본문 http url 링크만 있는 경우 탐지 \n",
    "def http_url_search(text):\n",
    "    pattern = r'http\\S+'\n",
    "    find_url = re.compile(pattern)\n",
    "    \n",
    "    return  bool(find_url.search(text))\n",
    "\n",
    "\n",
    "### 여기는 전처리 후보인데, 현재는 사용하지 않음 \n",
    "## 본문 전처리 1\n",
    "def body_preprocessing(body):\n",
    "    # 정규식을 사용해 불필요한 태그 제거\n",
    "    text = re.sub('<.*?>', '', body)\n",
    "    # [숫자] 패턴 제거\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    # [문자열] 패턴 제거\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    return text\n",
    "\n",
    "## 한글 체크하기 \n",
    "def contains_korean(text):\n",
    "    korean = re.compile('[ㄱ-ㅣ가-힣]+')\n",
    "    return bool(korean.search(text))\n",
    "\n",
    "# 본문 http url 링크 삭제하기 \n",
    "def text_http_del(text):\n",
    "    pattern = r'http\\S+'\n",
    "    text_without_urls = re.sub(pattern, '', text)\n",
    "    \n",
    "    return text_without_urls\n",
    "\n",
    "\n",
    "def extract_text_from_html(html):\n",
    "    # HTML 태그 제거\n",
    "    html = re.sub(r'<[^>]*>', '', html)\n",
    "    \n",
    "    # CSS 스타일 태그 제거\n",
    "    html = re.sub(r'<style.*>.*<\\/style>', '', html, flags=re.DOTALL)\n",
    "    \n",
    "    # 불필요한 공백 문자 제거\n",
    "    html = re.sub(r'\\s+', ' ', html)\n",
    "    \n",
    "    # 텍스트 추출\n",
    "    text = html.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_korean(text):\n",
    "    korean_pattern = re.compile('[^ㄱ-ㅣ가-힣]+')\n",
    "    korean_text = korean_pattern.sub(' ', text)\n",
    "    return korean_text\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "# 뉴스 본문 정규표현식 전처리 모듈 : 기자 나누기 \n",
    "def newsbody_preprocessing(body):\n",
    "    text_filter = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9&,\\.\\?\\!\\\"\\'-()\\[\\]\\{\\}]')\n",
    "    email_pattern = re.compile('^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$')\n",
    "    reporter_pattern = re.compile('[가-힣]{2,4},\\s?[가-힣]{2,4}\\s?기자|([가-힣]{2,4})\\s?기자')\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    \n",
    "    text = email_pattern.sub(' ', body)\n",
    "    reporter = reporter_pattern.search(text)\n",
    "    if reporter != None:\n",
    "        reporter = reporter.group(0)\n",
    "    # text = reporter_pattern.sub(' ', text)\n",
    "    text = text_filter.sub(' ', text)\n",
    "    text = doublespace_pattern.sub(' ', text).strip()\n",
    "    text = text.split(reporter)\n",
    "    text = text[0]\n",
    "    return text, reporter\n",
    "\n",
    "## pdf 로더 \n",
    "def extract_text_images_tables(pdf_path):\n",
    "    all_text = \"\"\n",
    "    images = []\n",
    "    tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text\n",
    "            all_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "            # Extract images\n",
    "            for image in page.images:\n",
    "                images.append(image)\n",
    "\n",
    "            # Extract tables\n",
    "            for table in page.extract_tables():\n",
    "                tables.append(table)\n",
    "    \n",
    "    return all_text, images, tables\n",
    "\n",
    "\n",
    "def extract_text_without_tables_images(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extracting raw text from the page\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            if page_text is None:\n",
    "                continue\n",
    "            \n",
    "            # Extracting tables from the page\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            # Filter out text that belongs to tables\n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    for row in table:\n",
    "                        for cell in row:\n",
    "                            if cell is not None:\n",
    "                                page_text = page_text.replace(cell, '')\n",
    "\n",
    "            # Adding the cleaned page text to the final text\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def pdf_loader(pdf_path):\n",
    "    #pdf_path ='./data/삼성전자샘플_005930.pdf'\n",
    "    # Extract text, images, and tables from the PDF\n",
    "    extracted_text = extract_text_without_tables_images(pdf_path)\n",
    "    # image / table can't\n",
    "\n",
    "    # Print the extracted text\n",
    "    pdf_text = repr(extracted_text)\n",
    "    \n",
    "    return pdf_text\n",
    "\n",
    "def get_pdf_paths(folder_path):\n",
    "\n",
    "    pdf_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_paths.append(os.path.join(root, file))\n",
    "    return pdf_paths\n",
    "\n",
    "def pdf_loader_by_folder(data_folder_path):\n",
    "    pdf_files = get_pdf_paths(data_folder_path)\n",
    "                         \n",
    "    page_list = []\n",
    "\n",
    "    for p in range(len(pdf_files)):\n",
    "        pdf_path = pdf_files[p]\n",
    "        page = pdf_loader(pdf_path)\n",
    "        page_list.append(page)\n",
    "    return page_list, pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58be76d-48b6-4474-89a4-acdcc16a32ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oosij",
   "language": "python",
   "name": "oosij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
